## 3. LES LIMITES DU MODÈLE MÉCANISTE

### CODE ET LANGAGE

Quelle que soit la nature de la pensée, on ne peut guère se la repré­senter que comme une grandeur continue du type du temps ou de la vitesse, Il serait tentant d'imaginer des « mentèmes » qui seraient ses unités constitutives, mais dans l'état actuel des connaissances, la théorie des quanta ne s'applique qu'à des grandeurs physiques.

Pour que la pensée puisse intervenir dans un système probabiliste comme un système informationnel, il est nécessaire de la transformer en une *grandeur discrète*, c'est-à-dire composée d'unités physiquement distinctes, Le pouvoir d'énonciation décrit plus haut sert précisément à cela. Il n'y a peut-être pas identité entre le langage et la pensée, mais les unités constitutives du langage (phonèmes, traces, etc.) sont les seules unités discrètes dont nous disposions pour quantifier l'information contenue dans la pensée.

Sans entrer pour le moment dans le détail, signalons tout de suite que la différence fondamentale qui distingue objectivement un code d'un langage, est qu'un code peut fonctionner indépendamment de toute pensée alors que le langage nous apparaît comme une modalité de la pensée.

Un code est un répertoire de signes conventionnels (ou pouvant être décrits comme conventionnels) répondant dans la mesure du possible aux conditions suivantes:
1. A *un signifiant* correspond un *seul signifié* et réciproquement. Il n'y a ambiguïté ni par *polysémie*, ni par *homonymie*. Il n'y a pas de signifiant inutile correspondant à un signifié déjà exprimé par ailleurs.
2. Les signifiants sont par eux-mêmes équiprobables et les variations de probabilité qui caractérisent les émissions de signes correspondant aux messages, dépendent uniquement des contraintes que font peser sur le signifié la nécessité d'inscrire telle ou telle information provenant d'une source toujours extérieure au système de codage.

Prenons le cas du code dit « alphabet Morse ». Il a constitué d'emblée une solution empirique quasi optimale, compte tenu des canaux dont on disposait à cette époque. Fonctionnant sur le principe du « tout ou rien », il résiste d'autant mieux au bruit qu'il peut utiliser le bruit comme support pour les signes. En effet il joue sur un système binaire de deux « états » qui peuvent éventuellement être la présence d'énergie (signal) ou l'absence d'énergie (silence) sur le conducteur. Il joue également sur la durée, ce qui lui permet de produire cinq *symboles* (éléments non signifiants de signes) très reconnaissables:
+ le point: signal de 1/24 de seconde
+ le trait: signal de 3/24 de seconde
+ le blanc court: silence de 1/24 de seconde
+ le blanc moyen : silence de 3/24 de seconde
+ le blanc long: silence de 6/24 de seconde

Si l'on devait considérer le manipulateur comme une source, on pourrait appliquer la formule de Shannon et dire que son entropie est de :

<a href="https://www.codecogs.com/eqnedit.php?latex=\textup{H}=&space;-\log_{2}\frac{1}{5}=2,32\textup{&space;bits}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textup{H}=&space;-\log_{2}\frac{1}{5}=2,32\textup{&space;bits}" title="\textup{H}= -\log_{2}\frac{1}{5}=2,32\textup{ bits}" /></a>

Mais la source n'est pas le manipulateur, c'est l'expéditeur du télégramme dont l'information a été énoncée (et non simplement codée) au moyen d'un langage lui-même énoncé au moyen de l'écriture qui est un langage de traces symboliques - en l'occurrence les lettres de l'al­phabet. Le code est obligé d'en tenir compte puisqu'il fonctionne au niveau des symboles-lettres, ce qui l'oblige à avoir un signal spécial pour les séparer (le blanc moyen). Mais de plus il ne peut pas ignorer que ces symboles-lettres servent à former des signes qui constituent le langage, ce qui l'oblige à avoir un signal spécial (le blanc long) pour séparer les mots, unités signifiantes constitutives du langage écrit.

L'implication du code dans le langage est donc beaucoup plus grande que ne le laisse supposer l'affirmation d'Elie Roubine. Le langage commande le code, ce qui veut dire que le *texte* commande le *message*.

C'est si vrai que l'inventeur du code Morse a dû tenir compte du fait qu'une des caractéristiques du langage est que les signes (et donc les symboles qui les constituent) ne sont jamais équiprobables. Afin d'obtenir une économie maximale et donc un rendement optimal, il a fait corres­pondre les signes de code les plus courts aux lettres les plus fréquentes en anglais. On notera que cette correspondance lie le code, au-delà du *langage*, à une *langue* spécifique. S'il fallait lui conserver le même rendement, cela supposerait un code différent pour chaque langue. Au niveau des signes, un code présente une variété de probabilités qui est fonction de ce qu'il code dans une langue, symboles ou signes, en l'occur­rence lettres ou mots.

Cela revient à dire que l'entropie du codeur est identique à celle de la source dont il code les messages et que cette entropie dépend de la langue dont se sert la source pour énoncer (et non coder) son information. Le code n'ajoute pas d'information, la langue en produit. Un exemple mettra en lumière la différence. Supposons un code:
ANGLAIS | FRANCAIS 
--- | --- | ---
*under*| *sous* 
*take*| *prendre* 
*-er*| *-eur* 

Soit une première combinaison des signes anglais *under-take*. Le codage donne *sous-prendre*, Si nous nous référons aux signifiés donnés non plus par le code, mais par la langue, nous constatons que le signifié d'*undertake* n'est pas le signifié (d'ailleurs douteux) de *sous-prendre*. Pour obtenir une adéquation, il nous faut donc « créer » un nouveau signe dans le code : *entre*, ce qui nous permet d'obtenir *undertake* = *entreprendre*. Si maintenant nous essayons la combinaison *undertak(e)-er*, le codage donne *entrepren(dre)-eur*. On notera que nous avons mis entre parenthèses des lettres rendues inutiles par les caractéristiques morpho­logiques et syntaxiques des deux langues, phénomène qui ne doit norma­lement pas se produire dans un code. Mais de plus, si nous posons l'équivalence *undertaker* = *entrepreneur*, nous constatons, en nous référant aux signifiés, que cette équivalence n'est pas vérifiée : *undertaker* contient davantage d'information qu'*entrepreneur* puisque, pour obtenir l'équivalence, il faudrait ajouter *de pompes funèbres*. Cette dernière information a été produite par le simple fait de la combinaison des signes. Il y a eu « création » d'information.

C'est là une propriété fondamentale des langues, spécifique de chacune d'elles, et c'est là ce qui différencie radicalement un codage d'une énon­ciation par le langage. 

Il en résulte que le régime des probabilités n'est pas le même dans le langage et dans le code quand ce dernier est considéré isolément.

D'abord les signes d'un langage (morphèmes pour le discours oral, mots ou *logogrammes* pour le texte écrit) et les symboles non signifiants qui les composent (phonèmes pour le discours oral, lettres pour le texte écrit au moyen d'un alphabet) *ne sont jamais équiprobables*. Le schéma de répartition de leurs probabilités, perçu statistiquement sur un ensemble de textes ou de discours, est spécifique non seulement de chaque langue, mais de chaque langage historique qui en découle. Ce schéma est étudié par la *statistique linguistique* dont il sera question plus loin. Nous savons déjà qu'il faut en tenir compte dans l'établissement d'un code efficace comme celui de Morse.

Il est plus difficile de tenir compte d'une autre caractéristique de l'énonciation linguistique, à savoir que *la probabilité des signes* varie au fur et à mesure que la source les émet. La probabilité de chaque signe dans un message est définie non seulement par sa probabilité générale dans la langue, mais aussi par l'apparition d'autres signes dans le même-message. Ainsi, en anglais écrit, la probabilité de U est de 0,0246, mais dès qu'un Q apparaît, elle est de 0,98, c'est-à-dire très voisine du 1 qui implique la certitude. Dans la même langue la proba­bilité du mot *than* est de 0,001, mais elle est de l'ordre 0,33 dans les cinq mots suivant un signe exprimant le comparatif.

Cela veut dire qu'une source employant le langage est une *source à mémoire* telle que nous avons défini plus haut ce type de source. La formule de Shannon ne lui est pas applicable.

Il faut ici pour comprendre esquisser rapidement une *théorie des jeux*. Soit un premier jeu constitué par un paquet de 52 cartes. La règle est de tirer une carte à chaque coup mais de remettre la carte dans le jeu après en avoir pris connaissance. L'entropie du jeu de cartes considéré comme source est, selon la formule de Shannon:

<a href="https://www.codecogs.com/eqnedit.php?latex=\textup{H}=&space;-\log_{2}\frac{1}{52}=5,7\textup{&space;bits}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textup{H}=&space;-\log_{2}\frac{1}{52}=5,7\textup{&space;bits}" title="\textup{H}= -\log_{2}\frac{1}{52}=5,7\textup{ bits}" /></a>

Chaque coup tiré est un message qui a une probabilité de 1/52 = 0,0192 et une information de 5,7 bits. Après chaque coup, l'entropie de la source reste intacte et la probabilité qu'a chaque carte de sortir reste la même. Si je tire vingt fois de suite la femme de cœur, mes chances de tirer la femme de pique ou tout autre carte sont toujours régies par la proba­bilité de 0,0192. La roulette est un jeu de ce type, ce qui montre la folie des joueurs à la recherche de « martingales », c'est-à-dire de changements de probabilité en fonction des coups tirés.

Soit maintenant un jeu constitué par le même paquet de cartes, mais où la règle est d'*écarter la carte* tirée après chaque coup. L'entropie initiale du paquet est de 5,7 bits et la probabilité de la première carte est de 0,0192. Mais au deuxième coup, l'entropie n'est plus que de <a href="https://www.codecogs.com/eqnedit.php?latex=-\log_{2}51&space;=&space;5,67\textup{&space;bits}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-\log_{2}51&space;=&space;5,67\textup{&space;bits}" title="-\log_{2}51 = 5,67\textup{ bits}" /></a> et la probabilité du coup de 1/51 = 0,0196. Au trentième coup, l'entropie est descendue à 4,52 bits et la probabilité du coup est de 0,043. Au cinquantième coup, l'entropie est tombée à 1,58 bit et la probabilité de la carte est de 0,333. Au cinquante-deuxième coup, l'entropie est nulle et la probabilité de la dernière carte est égale à l, ce qui équivaut à une certitude. Dans ce cas le plus général dans les systèmes physiques il y a *épuisement de l'entropie*. La formule de Shannon est applicable moyennant une loi selon laquelle quand une carte *c* de probabilité <a href="https://www.codecogs.com/eqnedit.php?latex=p_e" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_e" title="p_e" /></a> a été tirée, la probabilité <a href="https://www.codecogs.com/eqnedit.php?latex=p_n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_n" title="p_n" /></a> de chacune des *n* cartes restantes est augmentée d'une quantité égale à :

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{p_e.p_n}{1-p_e}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{p_e.p_n}{1-p_e}" title="\frac{p_e.p_n}{1-p_e}" /></a>

Dans le cas d'une source équiprobable de *s* signes et *m* le nombre de coups déjà tirés, la formule peut être simplifiée en disant qu'après le mème coup la probabilité de chacune des cartes restantes est de <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{s-m}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{s-m}" title="\frac{1}{s-m}" /></a> et l'entropie de la source de <a href="https://www.codecogs.com/eqnedit.php?latex=-\log_{2}\frac{1}{s-m}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-\log_{2}\frac{1}{s-m}" title="-\log_{2}\frac{1}{s-m}" /></a>

Notons que la formule de Shannon reste applicable sous sa forme développée quand les probabilités des signes sont inégales mais connues. Soit par exemple un jeu où le paquet de cartes est truqué de sorte qu'il y ait 26 dames de cœur et 13 dames de pique, les 13 autres cartes étant toutes différentes. La probabilité de la dame de cœur est de <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{26}{52}=\frac{1}{2}=0,5" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{26}{52}=\frac{1}{2}=0,5" title="\frac{26}{52}=\frac{1}{2}=0,5" /></a>, celle de la dame de pique est de <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{13}{52}=\frac{1}{4}=0,25" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{13}{52}=\frac{1}{4}=0,25" title="\frac{13}{52}=\frac{1}{4}=0,25" /></a>, celle de chacune des autres cartes de <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{52}=&space;0,0196" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{52}=&space;0,0196" title="\frac{1}{52}= 0,0196" /></a>. L'entropie initiale est donc de : 

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathrm{H}&space;=&space;-[(0,5&space;*-&space;1)&space;&plus;&space;(0,25&space;*-&space;2)&space;&plus;&space;13&space;(0,0196&space;*-&space;5,7)]&space;=&space;-[-0,5-0,5-1,45]&space;=&space;2,45\textrm{&space;bits}." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathrm{H}&space;=&space;-[(0,5&space;*-&space;1)&space;&plus;&space;(0,25&space;*-&space;2)&space;&plus;&space;13&space;(0,0196&space;*-&space;5,7)]&space;=&space;-[-0,5-0,5-1,45]&space;=&space;2,45\textrm{&space;bits}." title="\mathrm{H} = -[(0,5 *- 1) + (0,25 *- 2) + 13 (0,0196 *- 5,7)] = -[-0,5-0,5-1,45] = 2,45\textrm{ bits}." /></a>

Cette entropie reste constante dans le cas d'un jeu où l'on remet les cartes dans le paquet. Elle diminue selon la loi définie ci-dessus dans le cas où l'on écarte chaque carte après l'avoir tirée.

Nous avons jusqu'ici envisagé des jeux du type de la « bataille » où les cartes sortent au hasard. Nous pouvons envisager maintenant des jeux où les cartes qui sortent sont choisies en fonction d'une *stratégie* et font l'objet de choix délibérés et responsables de la part du joueur. Soit par exemple une main de whist comprenant 13 cartes différentes dont la liste est supposée connue par un observateur. Si les cartes sont jouées au hasard, cette main est pour l'observateur une source d'entropie <a href="https://www.codecogs.com/eqnedit.php?latex=-\log_2\frac{1}{13}=3,7\textrm{&space;bits}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-\log_2\frac{1}{13}=3,7\textrm{&space;bits}" title="-\log_2\frac{1}{13}=3,7\textrm{ bits}" /></a>
Mais la règle du whist est qu'elles soient jouées selon une idée directrice et en fonction de situations qui changent impré­visiblement à chaque coup joué. La probabilité pour une carte donnée de sortir varie selon le jeu des partenaires. La continuité même de la stratégie implique que le choix d'une carte soit lié aux choix effectués précédemment. Un bon joueur doit se souvenir de la séquence des coups joués non seulement par les autres, mais par lui-même. Autrement dit la main de whist est une source à mémoire.

Il est évident que la formule de Shannon n'est pas applicable. Au cours
de la partie, la probabilité de chaque carte varie de <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{13}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{13}" title="\frac{1}{13}" /></a> à 1(certitude) selon le rang dans lequel elle est jouée au cours de la partie et selon la prévisibilité de la séquence de jeu à laquelle elle appartient. L'entropie de la main peut varier de 3,7 bits dans le cas du jeu au hasard à 0 dans le cas où, pour un joueur exercé, la séquence de jeu s'impose de manière si rigoureuse qu'elle est indépendante des réactions des autres joueurs et que chaque carte a une probabilité égale à 1. C'est ce qui arrive quand on « abat son jeu ».

Le fonctionnement du langage est analogue à celui d'une partie de cartes, mais non identique. Rappelons quelques-unes des différences.
1. Alors que les cartes sont en nombre limité et connu, le langage peut toujours ajouter de nouveaux signes plus ou moins prévisibles à la liste « officielle » que lui fournit la langue.
2. Alors que, dans les cartes, les signifiants (figure, couleur) sont liés par une loi stricte et invariable, définie pour chaque jeu, au signifié (valeur que prend la carte par rapport aux autres pendant la partie), il est toujours possible dans le langage de faire varier entre le signifiant, et le signifié d'un mot le lien que lui assignent les lois de la langue.
3. Alors que, dans une partie de cartes, les règles sont strictement codifiées, celles de la langue laissent beaucoup plus de latitude pour l'élaboration des séquences de signes et le langage peut éventuellement les violer.
4. Alors que, dans un jeu du type du whist, les cartes sont écartées après chaque coup, dans le langage les mots sont remis en jeu après chaque énoncé. Le whist appartient au deuxième type de jeu que nous avons évoqué. L'épuisement de l'entropie des mains définit la fin d'un rob. Pour continuer il faut recréer l'entropie en redistribuant les cartes. Les robs sont liés entre eux par une mémoire sommaire qui est la marque, mais au bout d'un certain nombre de robs, la partie s'achève. Au contraire dans le cas du langage, la partie ne s'achève jamais : tous les messages linguistiques depuis que l'homme sait parler s'enchaînent les uns aux autres. On peut seulement noter des « phases » (un peu l'équi­valent des robs) séparées par des moments où la probabilité des mots tend vers la certitude, comme par exemple à la fin des lettres les formules de politesse stéréotypées.

C'est cette dernière caractéristique que les théoriciens ont exploitée
pour construire un *modèle mathématique approximatif*.

Une première hypothèse, émise par le mathématicien russe Andreï Andreïevitch Markov en 1907, est que dans le langage toute la mémoire des messages antérieurs déterminant la probabilité du message à émettre est contenue dans le dernier message émis. On appelle une source définie par cette caractéristique une *source markovienne* et les chaînes de messages liés qu'elle engendre sont les *chaînes de Markov*.

Une application de l'hypothèse de Markov consiste à considérer non plus la probabilité d'un signe ou d'un symbole isolé, mais sa proba­ bilité quand il est lié à un autre signe ou symbole le précédant. Nous avons vu plus haut que dans le cas des digrammes comme QU ou des associations comme le comparatif plus *than*, ces probabilités liées sont une des caractéristiques de l'énoncé linguistique. Il est d'ailleurs évident que le couple formé par le message-mémoire et le message à émettre a lui-même une probabilité liée au message immédiatement antérieur. Cela conduit à considérer non plus des digrammes, mais des trigrammes, puis des tétragrammes, puis des séquences de *n* lettres, puis, au-delà des lettres, des séquences de mots de plus en plus longues.

La deuxième hypothèse qui a été faite est que si la séquence est assez longue, après *n* messages, l'influence d'un message-mémoire M sur la probabilité du message M + *n* + 1 est négligeable, La langue définie par l'ensemble des messages qu'elle a servi à émettre ou par un sous-ensemble suffisamment grand de cet ensemble, ne perd pas la mémoire, mais cette mémoire se structure au point qu'on puisse affirmer sans grand risque d'erreur que la probabilité de la lettre E dans un texte anglais est de 0,13 et que celle du mot *dieu* dans une conversation en français parlé contemporain est de l'ordre de 0,00025, la même que celle de *cinéma*.

Cela permet d'attribuer à toute source de langage les trois propriétés
mathématiques suivantes :

1. C'est une source *homogène*, ce qui veut dire que l'émission des messages est indépendante du temps et qu'il n'y a pas d'instant initial privilégié (où l'on pourrait par exemple définir une entropie).
2. C'est une source *stationnaire*, ce qui veut dire que la loi qui règle l'émission des messages n'est, pas une loi initiale, mais une loi choisie parce qu'elle rend compte au mieux des observations faites au cours d'un long fonctionnement.
3. C'est une source *ergodique* (ou *régulière*), ce qui veut dire qu'elle peut produire un nombre ou un ensemble infini de séquences infinies de messages où la répartition statistique des probabilités reste stable, En particulier il ne peut jamais arriver qu'un message puisse se déter­miner lui-même comme le message suivant avec une probabilité de 1, ce qui conduirait la source à se répéter indéfiniment et à épuiser d'un coup toute son entropie.

Moyennant ces trois propriétés, il est possible de ramener le fonc­tionnement du langage à un modèle mathématique. Mais il est visible que ce résultat est une sorte de « voie de garage » où les théoriciens de la télécommunication ont voulu se débarrasser d'un problème gênant. Au-delà de ces hypothèses s'étend tout l'immense champ de la *statistique linguistique* dont Markov précisément a été le pionnier. C'est en ana­lysant en 1913 la répartition des voyelles et des consonnes dans l'*Eugène Onéguine* de Pouchkine que Markova mis au point sa théorie des chaînes qui maintenant possède d'innombrables applications dans tous les domaines des sciences humaines.

Sans aborder un sujet qui sera traité plus loin, on comprendra d'autre part que les objectifs pratiques des théoriciens de la télécommu­nication puissent être très différents de ceux de théoriciens préoccupés par d'autres aspects de la communication, et en particulier par la commu­ nication littéraire et artistique.

La description de la langue comme une source homogène, sta­tionnaire et ergodique est précisément ce que Roland Barthes a appelé le « *degré zéro de l'écriture* », c'est-à-dire un état hypothétique où, à la suite d'un long fonctionnement, il devient impossible à l'écrivain d'énoncer son originalité au moyen du langage écrit<sup id="a1">[1](#f1)</sup>.

Abraham Moles a donné une description quantitative de l'origi­nalité dans son livre *Théorie de l'information et perception esthétique*<sup id="a2">[2](#f2)</sup>. Partant des hypothèses mécanistes ci-dessus, il définit *a priori* l'origi­nalité comme mesurée par la quantité d'information au sens shannonien du mot.

Mais il n'est pas du tout certain que ce qui est recherché par la communication esthétique et littéraire soit - du moins exclusivement - ce qui est le souci majeur des théoriciens de la communication, à savoir faire passer par le canal le maximum d'information avec le maxi­mum de fidélité et le maximum d'économie de temps et d'énergie. Il se peut que les bruits soient essentiels. En tout cas on ne peut rendre compte de ce genre de communication en adoptant l'attitude de l'employé des Postes à qui les motivations de l'expéditeur du télégramme et les réactions du destinataire sont indifférentes.

Rien ne l'illustre mieux que le fameux sketch d'Yves Montand, *le Télégramme*, où l'on voit un homme tenter d'envoyer un message d'amour à une femme par l'intermédiaire du service des télégrammes téléphonés. La codification pourtant élémentaire à laquelle se livre la préposée du service assurera la transmission intégrale de l'information contenue dans le message et à vrai dire c'est une bien petite quantité d'information au sens probabiliste du mot mais éliminera totalement ce qui était l'essentiel du message pour l'expéditeur aussi bien que le destinataire, à savoir la tendresse.

Au sens que nous avons donné à ce mot jusqu'ici, il n'y a pas d'infor­mation dans cette tendresse puisque les deux intéressés savent bien qu'ils s'aiment. C'est ce qu'on appelle un message *redondant*. Faut-il admettre que lorsqu'on a dit à une -persorme qu'on l'aime une bonne fois, il devient oiseux de le répéter? Les services des postes perdraient à ce compte une bonne partie de leur clientèle.

Il est raisonnable de penser que l'information définie comme indé­termination, commode pour la quantification du canal, ne recouvre qu'une partie d'une conception plus générale de l'information : celle qui concerne sa communication par des moyens mécaniques. Dans le cas du télégramme de Montand, l'information n'est pas dans le carac­tère plus ou moins imprévisible du contenu du message, mais dans le caractère entièrement prévisible de l'acte rituel qui consistait à l'envoyer et dans la forme redondante employée pour le rédiger.

### LA REDONDANCE

Pour conclure cette description des limites du modèle mécaniste, il ne sera pas inutile de dire quelques mots de la notion de *redondance*. 

Quantitativement, la redondance mesure l'«inutilité» d'un symbole, d'un signe ou d'un message.

Elle découle de deux propriétés de la formule de Shannon :
1. L'entropie d'une source augmente en fonction directe du nombre des éléments qu'elle contient et dont la probabilité d'apparition détermine l'entropie. Dans le cas d'éléments équiprobables, elle augmente de <a href="https://www.codecogs.com/eqnedit.php?latex=1&space;=&space;-\log_2\frac{1}{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?1&space;=&space;-\log_2\frac{1}{2}" title="1 = -\log_2\frac{1}{2}" /></a> pour deux éléments à <a href="https://www.codecogs.com/eqnedit.php?latex=-\log_2\frac{1}{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-\log_2\frac{1}{n}" title="-\log_2\frac{1}{n}" /></a> pour *n* éléments. Ainsi une pièce de monnaie a une entropie de 1 bit, un dé une entropie de 2,585 bits, un jeu de 32 cartes une entropie de 5 bits, un jeu de 52 cartes une entropie de 5,7 bits, etc.
2. *L'entropie d'une source diminue en fonction directe de la dissy­métrie des probabilités de chacun des éléments*. Elle est maximale et égale à <a href="https://www.codecogs.com/eqnedit.php?latex=-\log_2\frac{1}{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-\log_2\frac{1}{n}" title="-\log_2\frac{1}{n}" /></a> pour *n* éléments quand les éléments sont équiprobables, elle est égale à 0 quand un des éléments a une probabilité égale à l et donc les autres une probabilité égale à O. Ainsi imaginons une boîte d'où on doit tirer un jeton blanc, bleu, jaune ou rouge. Si la boîte
contient 2 jetons de chaque couleur, les couleurs des jetons tirés sont équiprobables : <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{4}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{4}" title="\frac{1}{4}" /></a>. L'entropie de la boîte est de <a href="https://www.codecogs.com/eqnedit.php?latex=-\log_2\frac{1}{4}=2\textrm{&space;bits}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-\log_2\frac{1}{4}=2\textrm{&space;bits}" title="-\log_2\frac{1}{4}=2\textrm{ bits}" /></a>. Si la boîte contient 4 jetons blancs, 2 bleus, 1 jaune et 1 rouge, l'entropie est de :

<a href="https://www.codecogs.com/eqnedit.php?latex=\textrm{H}=-(-\frac{1}{2}*\log_2\frac{1}{2})&plus;(-\frac{1}{4}*\log_2\frac{1}{4})&plus;2*(-\frac{1}{8}*\log_2\frac{1}{8})=1,75\textrm{&space;bit}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textrm{H}=-(-\frac{1}{2}*\log_2\frac{1}{2})&plus;(-\frac{1}{4}*\log_2\frac{1}{4})&plus;2*(-\frac{1}{8}*\log_2\frac{1}{8})=1,75\textrm{&space;bit}" title="\textrm{H}=-(-\frac{1}{2}*\log_2\frac{1}{2})+(-\frac{1}{4}*\log_2\frac{1}{4})+2*(-\frac{1}{8}*\log_2\frac{1}{8})=1,75\textrm{ bit}" /></a>

Si la boîte ne contient que des jetons blancs, l'entropie est
égale à zéro.

Pour toute source on peut donc définir, en fonction des éléments N qu'elle contient, une *entropie maximale* égale à <a href="https://www.codecogs.com/eqnedit.php?latex=-\log_2\frac{1}{\textrm{N}}=\log_2\textrm{N}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-\log_2\frac{1}{\textrm{N}}=\log_2\textrm{N}" title="-\log_2\frac{1}{\textrm{N}}=\log_2\textrm{N}" /></a>.

D'autre part on peut définir pour la même source une *entropie effective* H*s* qui dépend de la loi de probabilité quelconque qu'on lui suppose ou qu'on lui calcule après expérience.

On peut alors définir pour cette source une *entropie relative h* par le rapport : <a href="https://www.codecogs.com/eqnedit.php?latex=h&space;=\frac{\textrm{H}s}{\log_2\textrm{N}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h&space;=\frac{\textrm{H}s}{\log_2\textrm{N}}" title="h =\frac{\textrm{H}s}{\log_2\textrm{N}}" /></a> qui a une valeur de 1 si l'entropie effective et l'entropie maximale sont égales. Elle est d'autant plus faible que l'écart entre les deux est plus grand, c'est-à-dire que chaque message contient moins d'information.

La *redondance* R est alors R = 1 - *h*. Elle peut prendre des valeurs comprises entre 0 dans le cas d'un message parfaitement « efficace » et 1 dans le cas d'un message parfaitement « inutile ».

On voit les services que la notion de redondance peut rendre aux théoriciens de la communication en particulier au niveau des symboles élémentaires.

D'abord elle peut servir à la recherche de l'économie en permettant d'abréger les messages. Un télégramme rédigé ainsi :
```
NOUS ARRIVERONS DEMAIN MARDI TROIS JUIN
```
n'apporte pas plus d'information en 34 signes que le suivant en 13 signes:
```
ARRIVONS DEMAIN
```

La terminaison ONS suffit à signifier la première personne du pluriel et DEMAIN suffit à signifier d'une part le futur, d'autre part la date, compte tenu des indications de service que comporte tout télégramme. 

C'est la redondance de l'écriture alphabétique (variable selon les langues) qu'utilisent (en l'éliminant) les divers systèmes de sténographie et de tachygraphie.

Un des premiers soucis des théoriciens de la télécommunication a été d'éliminer au maximum la redondance des codes qu'ils emploient et d'augmenter ainsi leur *efficacité*. Cette efficacité est le rapport *y* de la quantité d'information apporté par le message à coder au nombre des symboles binaires servant à la coder. La redondance du code est alors de R = 1 - *y*.

Ce qu'on appelle le premier théorème de Shannon fixe les conditions de l'*efficacité maximale* d'un code binaire servant à coder les messages d'une source linguistique dont les caractéristiques ont été décrites plus haut. Il montre que pour toute source de ce type il existe un codage irré­ductible obtenu en codant les symboles ou signes en « mots » (ou séquences) suffisamment longs, qui permet de s'approcher autant qu'on veut d'une valeur de 1 pour y et de 0 pour R.

Mais les théoriciens de la télécommunication n'ont pas une attitude entièrement négative envers la redondance .. Certes l'efficacité permet d'accélérer la vitesse de transmission et donc de faire des économies, mais, compte tenu des contraintes matérielles du canal, ces économies ne sont pas toujours payantes.

Par exemple nous savons déjà que seules sont transmises les modu­lations admises par la bande passante d'un canal. Les autres sont éli­minées. Si le message reste encore intelligible, il faut admettre que les modulations éliminées étaient redondantes. Il y a donc là un choix à faire : ou bien chercher une bande passante aussi large que possible, ce qui est coûteux en équipement et en énergie, ou bien donner au message suffisamment de redondance pour qu'il s'accommode d'une bande étroite, ce qui est coûteux en temps. La deuxième solution est celle qui est choisie en général pour la télécommunication, la première étant préférée dans le cas par exemple où il faut transmettre de la musique, les messages musicaux étant les moins redondants de tous et exigeant une très haute fidélité.

Dans son deuxième théorème, considéré comme le théorème fonda­mental de la théorie de l'information, Shannon s'est préoccupé du *critère de fidélité* d'un canal bruyant. Il ne s'agit plus seulement des distorsions dues à la largeur de la bande passante, mais, d'une manière générale, des pertes d'information, c'est-à-dire des erreurs de décodage. Shannon montre que tant que la quantité d'information à transmettre dans un temps T (débit) reste inférieure à la *capacité du canal*, c'est-à-dire la quantité d'information qu'il peut transmettre dans le temps T, il est possible de rendre la probabilité d'erreur aussi petite qu'on veut.

Pour définir le *critère de fidélité* ou *transinformation* d'un canal, il faut supposer à son entrée (codage) et à sa sortie (décodage) un dispo­sitif permettant la prédiction des signaux et l'évaluation de leur proba­bilité. Si l'on appelle <a href="https://www.codecogs.com/eqnedit.php?latex=p_s" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_s" title="p_s" /></a> la probabilité d'apparition d'un signal donné (symbole ou signe) à la source et <a href="https://www.codecogs.com/eqnedit.php?latex=p_d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_d" title="p_d" /></a> la probabilité de réception par le destinataire du signal correspondant, on peut définir une probabilité conditionnelle <a href="https://www.codecogs.com/eqnedit.php?latex=p_{s/d}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{s/d}" title="p_{s/d}" /></a> qu'un signal donné de la source ait été émis quand le destinataire reçoit le signal correspondant. Cela permet de calculer une entropie <a href="https://www.codecogs.com/eqnedit.php?latex=\textrm{H}_{s/d}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textrm{H}_{s/d}" title="\textrm{H}_{s/d}" /></a> qui est l'entropie de la source vue par le destinataire. L'entropie réelle de la source <a href="https://www.codecogs.com/eqnedit.php?latex=\textrm{H}_{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textrm{H}_{s}" title="\textrm{H}_{s}" /></a> étant connue, la transinformation du canal se définit comme <a href="https://www.codecogs.com/eqnedit.php?latex=\textrm{I}=\textrm{H}_{s}-\textrm{H}_{s/d}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textrm{I}=\textrm{H}_{s}-\textrm{H}_{s/d}" title="\textrm{I}=\textrm{H}_{s}-\textrm{H}_{s/d}" /></a>.

I peut varier entre les valeurs <a href="https://www.codecogs.com/eqnedit.php?latex=\textrm{H}_{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textrm{H}_{s}" title="\textrm{H}_{s}" /></a> (canal sans bruit) et 0 (canal entièrement bruyant et donc inutilisable). La capacité d'un canal est la valeur maximale que peut prendre I pour ce canal. Shannon a montré qu'il existe une loi de probabilité d'émission et donc de codage qui maximalise I.

Il est bien évident que ce codage optimal implique une certaine quantité de redondance qui est fonction de la capacité du canal. On retrouve ici le jeu de la recherche d'équilibre entre diverses variables en vue d'un rendement optimal. La redondance est une de ces variables et possède donc une valeur positive.

On trouve dans la vie courante une utilisation constante de la redondance qui est, selon les exigences de la communication, augmentée ou diminuée.

Par exemple, lorsqu'il est vital que le message soit exactement et entièrement reçu, on augmente délibérément la redondance par la répé­tition du message en employant au besoin deux codes différents. C'est ainsi que lorsqu'on rédige un chèque, on indique la somme une fois en lettres et une autre fois en chiffres. La « collation » d'un télégramme correspond au même souci. On peut également, suivant en cela le premier théorème de Shannon, augmenter pour un signifié la longueur du signi­ fiant et donc sa redondance. C'est ce que font les employés des postes quand pour A, B, C ... , ils disent: Anatole, Berthe, Célestin ... , ou ce que font les aviateurs quand ils emploient le code: Alpha, Bravo, etc.

Inversement il peut arriver que lorsque la complication formelle d'un signifiant est telle qu'il est peu maniable (ce qui est une cause d'erreur au niveau de la source), on en élimine les symboles redondants pour n'en garder que les *traits pertinents*, c'est-à-dire les éléments qui le rendent aisément reconnaissable. C'est ce qui se produit quand on simplifie une orthographe. L'exemple le plus typique est celui des caractères chinois classiques dont la redondance a été délibérément réduite par le Japon et par la Chine populaire aux fins d'utilisation dans la presse et le livre.

C'est ainsi que le signe *xué* (apprendre) &#23416; (originellement un enfant sous un toit) est devenu &#23398; sans rien perdre de sa valeur informationnelle.

On pourra arguer qu'il a perdu tout de même quelque chose de sa valeur esthétique, or l'esthétique est incontestablement une forme de la communication. Il est des cas où la même opération peut réellement diminuer la quantité d'information transmise. L'ancien signe *ma* (cheval) &#39340;  rappelait vaguement un cheval dont il possédait certains des traits pertinents (les pattes, la crinière). Sous sa forme simplifiée &#39532; il reste identifiable, mais ne renvoie plus qu'au phonème *ma*.

Du point de vue du théoricien de la télécommunication, ce résultat est satisfaisant. Il n'est pas certain qu'il le soit pour qui veut envisager la communication d'une manière plus générale.

### HARDWARE ET SOFTWARE

Les remarques qui précèdent montrent assez clairement que ce qui se dégage du schéma mécaniste des théoriciens de la télécommunication est *une théorie partielle de l'information*. Il est visible que leur problème est le canal et non la source, mais qu'ils ne peuvent ignorer la source dans la mesure où elle conditionne les messages que le canal doit trans­mettre.

Or cette source est inévitablement une pensée humaine ou tout au moins de type humain, car on a rarement vu un insecte se servir du téléphone ou du télégraphe. La notion d'information est liée à toute forme de pensée, même rudimentaire, pourvu qu'elle soit capable de prévoir, donc de supputer des probabilités, même si cette prévision n'est que la répétition d'un modèle qui se trouve imprévisiblement interrompue. En ce sens on peut dire qu'il y a une « pensée » de la molécule vivante. Le schéma mécaniste s'accommode très bien de ces dernières formes de pensée qui sont liées à des systèmes discrets de signes par des codes relativement faciles à établir. C'est ainsi qu'on a pu apprendre le « langage » gestuel des abeilles, c'est-à-dire en fait qu'on a pu codifier leurs gestes.

Mais la pensée de type humain (dans laquelle il faut inclure celle de certains animaux supérieurs) a un pouvoir particulier d'invention qui se traduit au niveau du corps par la faculté de dominer les déterminations de l'environnement par la main et par la parole les deux caracté­ristiques de l'homme. La main lui permet de manipuler l'environnement au moyen de l'outil et la parole lui permet d'énoncer l'environnement par le langage. Ici il n'est évidemment plus question de code, ou du moins l'essentiel du comportement est dans les violations novatrices des codes.

Or un canal de télécommunication est par définition un outil au service d'un langage. On comprend le dilemme des théoriciens dont tous les modèles mathématiques et tous les schémas de base sont issus de la mécanique : ou bien il leur faut se résigner à ne donner qu'une analyse partielle rejetant dans l'aléatoire tout ce qui est non mécanique, ou bien il leur faut envahir et en quelque sorte coloniser les territoires des diverses sciences de la vie et des diverses sciences de l'homme, y compris celles qui se trouvent à la limite de la connaissance empirique et de la spéculation philosophique. Les spécialistes de ces dernières sciences ont, parfois un peu abusivement, appelé cette invasion, séduits qu'ils étaient par la commodité et la clarté des schémas mécanistes. Il y a un emploi métaphorique de la théorie mathématique de l'infor­mation qui a fait des ravages.

La plupart des théoriciens sont sortis du dilemme en formulant implicitement ce qu'on peut appeler l'*hypothèse de service*, c'est-à-dire en adoptant, comme dit E. Roubine, l'attitude de l'employé des Postes. Le service à rendre est de transmettre le plus rapidement, le plus écono­miquement et surtout le plus fidèlement possible au destinataire l'infor­mation contenue dans les messages de la source, c'est-à-dire d'annuler aux yeux du premier les imprévisibilités de la seconde autant que le permettent les signes qu'elle rend manifestes et confie au canal.

Or il n'est pas du tout certain que ce soit nécessairement ce service­ là que la source et le destinataire attendent de l'appareil de commu­nication. Dans le cas du télégraphe ou du téléphone, l'hypothèse est assez vraisemblable, mais même dans ce cas elle ne tient aucun compte des stratégies internes et réciproques des communicants, ni de l'impli­cation de ces derniers dans un réseau d'actes de communication et d'actes tout court où ils sont des relais plus ou moins fidèles.

On peut faire une analyse informationnelle de la fameuse dépêche d'Ems qui déclencha la guerre de 1870, mais la méthode mathématique ne donnerait qu'un résultant insignifiant. Elle traiterait Guillaume Ier comme la source et la manipulation de Bismarck comme un bruit, alors qu'en fait la vraie source est Bismarck. D'autre part l'acheminement du message par deux chenaux successifs : télégraphe entre Ems et Berlin, presse entre Berlin et Paris, est moins un acte de communication qu'un « coup » joué dans un *Kriegspiel* qui implique entre autres la situation politique de la Prusse en Allemagne, la situation politique du régime impérial en France, les calculs intellectuels de Bismarck et les réactions émotionnelles du gouvernement et du Parlement français.

Autrement dit, le schéma mécaniste ignore délibérément les aspects psychologiques et sociologiques de l'avant-canal et de l'après-canal. La théorie mathématique de l'information, surtout depuis Shannon, est certes un apport décisif et inappréciable à la compréhension du fonctionnement de l'appareil de communication. On ne peut en aucun cas se passer d'elle, car seule elle permet de contrôler tout ce qu'il y a de physique dans la communication et d'en tirer le meilleur parti. Ce contrôle qui n'était pas essentiel tant que l'homme en était réduit pour communiquer à ses propres moyens énergétiques, est devenu vital depuis qu'il est en mesure d'utiliser l'énergie artificielle et donc des machines.

Mais maîtriser ce qu'on a appelé irrespectueusement la « quin­caille » en anglais *hardware* n'est pas un but en soi. C'est simple­ment un moyen d'assurer un bon fonctionnement du reste. Or le reste, par un parallélisme de codage plus que de langage, qui vient tout natu­rellement en anglais, a été baptisé *software*. C'est un mot difficilement traduisible pour lequel Louis Armand a proposé le terme nettement plus noble de « mentaille ».

Cette terminologie n'est pas seulement irrespectueuse. Elle est révélatrice d'une idéologie pour le moins inquiétante. L'humanité a été longue à se libérer de la dichotomie mutilante esprit-matière. Le geste de Paracelse brûlant ses livres au cours de sa leçon inaugurale de l'Université de Bâle en l528 voulait proclamer son refus d'accepter cette dichotomie. On a méconnu Paracelse, mais c'est sur ce refus qu'a été bâtie toute la science moderne et en particulier la science de l'homme.

La civilisation mécanique et surtout la société industrielle fondée sur le profit, qui lui a donné naissance, revient volontiers à l'antique dichotomie, car elle lui permet de proclamer l'innocence de la machine, ou du moins sa neutralité dans les débats humains. 

C'est ce que voudrait faire accepter ce que nous appelons l'hypo­ thèse de service: l'employé des Postes qui achemine le télégramme n'est pas responsable de ce qui se passe dans l'esprit de l'expéditeur ou du destinataire.

Et pourtant, en tant que représentant de l'administration des Postes, il est responsable de l'information (au sens étymologique du mot) de leur pensée. Le formulaire avec ses cases (indications de service, adresse, texte, signature), le langage télégraphique avec ses contraintes sanc­tionnées par un paiement plus ou moins lourd, la censure même du message qui existe dans certains pays, comme aux États-Unis où c'est un délit d'employer les moyens de communication à certaines fins, .tout cela contribue à « informer » la pensée de l'expéditeur, c'est-à-dire à la transformer de source continue incommunicable en source discrète communicable.

Il est évident que la chose est inévitable, mais elle nous indique tout au moins que c'est une duperie d'établir une frontière entre le canal et ce qui vient avant et après lui. La contrainte du canal préforme le message. Tout ce qui n'est pas pertinent (*relevant* , dit-on en anglais) à cette préformation est considéré comme bruit et le bruit est soit éli­miné, soit utilisé pour confirmer ce qui est pertinent. Or précisément, ce qui fait l'originalité irremplaçable et le prix inestimable de la pensée humaine, c'est le pouvoir de non-pertinence, l'imprévisibilité vraie de l'énonciation et non plus l'imprévisibilité domptée à l'intérieur d'un système probabiliste. A la proposition binaire « de deux choses l'une », c'est le pouvoir de répondre « la troisième ».

Ce pouvoir porte un nom peu scientifique, mais important, car il s'appelle la liberté. Une caricature de 1787 montre le ministre Calonne demandant à l'Assemblée des Notables:
>« A quelle sauce voulez-vous être mangés?
>- Nous ne voulons pas être mangés! répondaient les notables.
>- Vous sortez de la question! » s'exclamait Calonne, jouant en conscience son rôle de commis à la communication entre les sujets et le pouvoir.

Le canal refusait l'information et Calonne n'avait pas tort: les notables sortaient de la question. Ce fut fort heureux d'ailleurs car, désespérant de trouver un fil qui eût la capacité nécessaire, les intéressés, deux ans plus tard, se firent comprendre en déclenchant la Révolution .

***
. ---- _. -
<b id="f1">1</b>R, BARTHES, *Le degré zéro de l'écriture*, Paris, Seuil, 1953.[↩](#a1)
<b id="f2">2</b>A. MOLES, professeur à l'Université de Strasbourg, sera plusieurs fois cité dans cet ouvrage, car sa formation de physicien et de sociologue, acquise en partie aux Etats-Unis, ainsi que sa forte culture artistique et littéraire, font de lui un des pionniers d'une théorie *générale* de l'information. Son livre, *Théorie de l'information et perception esthétique*, publié en 1958, a été réédité sous une forme révisée en 1972. Voir surtout les sections 1-1-5 et 1-1-6, « Information et originalité » et « La mesure de l'originalité ».[↩](#a2)
