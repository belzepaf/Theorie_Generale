## 2. LE TEMPS DES INGÉNIEURS

### THERMODYNAMIQUE ET ENTROPIE

Les théoriciens de la télécommunication se sont servis d'outils, mathématiques déjà existants, en l'occurrence ceux de la thermodynamique et de la mécanique statistique.

Première source artificielle d'énergie applicable à l'industrie, la machine à vapeur avait déjà posé des problèmes de rendement aux chercheurs. En 1824, le physicien Sadi Carnot publia une petite brochure intitulée: *Réflexions sur la puissance motrice du feu et les machines propres à développer cette puissance*<sup id="a1">[1](#f1)</sup>. Comme plus tard celle de Shannon, cette mise au point était l'aboutissement d'un grand nombre de recherches et de tâtonnements antérieurs. On y trouve formulé, outre le principe de l'équivalence du travail et de la chaleur, ce qui est appelé le « principe de Carnot » et qui s'énonce maintenant de la manière suivante: « Une machine thermique ne peut fonctionner sans qu'il passe de la chaleur d'une source chaude sur une source froide. » 

En 1850 le physicien allemand Rudolf Emmanuel Clausius<sup id="a2">[2](#f2)</sup> donna de ce principe la formulation suivante: « La chaleur ne peut passer d'elle-même d'un corps froid à un corps chaud. » De là il tira en 1876 la notion d'*entropie*.
C'est une quantité qui s'exprime par l'intégrale

<a href="https://www.codecogs.com/eqnedit.php?latex=\int&space;\frac{dQ}{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\int&space;\frac{dQ}{T}" title="\int \frac{dQ}{T}" /></a>

où dQ est la quantité de chaleur cédée par une source chaude à un système mécanique et T la température absolue à laquelle s'effectue l'opération. Dans le cas d'une machine parfaite fonctionnant en cycle fermé, cette quantité reste constante: le gaz contenu dans le, cylindre d'une machine perd de la chaleur à mesure qu'il fournit du travail en poussant le piston, mais si l'on restitue ce travail en repoussant le piston en sens inverse, il récupère la même quantité de chaleur.

Si la chaleur est cédée sans l'interposition d'un système mécanique
qui la transforme en travail, l'entropie augmente. C'est ce qui se passe,
par exemple, quand le gaz cède aux parois du cylindre une partie de sa chaleur qui est rayonnée vers l'extérieur et devient irrécupérable, puis qu'elle ne peut revenir d'elle-même de la source froide constituée par l'air ambiant vers la source chaude constituée par le cylindre.

En aucun cas l'entropie ne peut diminuer, ce qui revient à dire que si l'énergie se conserve, elle prend des formes de moins en moins utilisables. La chaleur par exemple n'est jamais entièrement transformable en énergie mécanique. C'est ce qu'on appelle la dégradation de l'énergie.

On voit immédiatement le parti qui peut être tiré de la notion d'en­tropie pour la recherche d'un meilleur rendement des machines thermiques. Il est remarquable qu'une telle théorie n'ait été formulée que deux siècles après l'apparition des premières machines à vapeur et un siècle après que James Watt<sup id="a3">[3](#f3)</sup>, pressentant par la pratique ce qui devait devenir le principe de Carnot, eut donné à ce type de machine un rendement assez acceptable pour que des applications industrielles puissent etre mises en œuvre.

On peut d'ailleurs noter par parenthèse que le perfectionnement le plus important apporté par Watt à la machine à vapeur est le tiroir. C'est un dispositif mécanique que l'on peut décrire comme un système de communication et plus précisément de *feedback*, c'est-à-dire de réin­jection de l'information. Pièce coulissante commandée par le mouvement du piston, le tiroir commande alternativement l'admission de la vapeur de part et d'autre du piston, produisant ainsi ce qu'on appelle le « double effet ». En d'autres termes, on peut dire que le tiroir, utilisant un vecteur mécanique pour transmettre l'information, « fait savoir » à la machine que, le piston ayant achevé sa course, un état d'équilibre est atteint et lui « donne l'ordre » d'injecter la vapeur sur l'autre face, ce qui replace le système en état de déséquilibre et amorce un nouveau cycle.

L'ingénieur a donc très largement précédé le théoricien. Il y a là un schéma sur lequel il est utile d'insister car nous le retrouvons dans le cas de la technologie de l'information. L'invention proprement dite vient souvent d'une idée fortuite comme la marmite de Denis Papin, idée qui est parfois le sous-produit d'un bricolage scientifique comme le cohéreur de Branly. Puis vient le technicien, l'ingénieur, le praticien - Watt pour la machine à vapeur, Edison pour l'électricité, Marconi pour les ondes hertziennes - qui invente le dispositif capable de donner à l'idée initiale un rendement acceptable et donc susceptible d'appeler l'investissement des capitaux. Notons que Gutenberg se situe très exactement à ce stade dans le développement de l'imprimerie.
Dès lors, c'est l'investissement qui commande, exigeant une rémunération et donc un rendement toujours accrue. A partir de la découverte de l'énergie artificielle et l'invention de Gutenberg a stagné plus de deux siècles jusqu'à ce moment , la technologie devient suffisamment raffinée et complexe pour justifier des investissements spéciaux consacrés à la recherche systématique du rendement.
C'est alors qu'entrent en scène les théoriciens et que peut vraiment se développer une recherche fondamentale utilisant l'outil mathématique.

### GÉNÉRALISATION DE L'ENTROPIE

Le souci principal des théoriciens mathématiques est la généralisation, c'est-à-dire le processus par lequel la formule descriptive d'un phénomène particulier (comme l'intégrale de Clausius) peut être ramenée à un modèle plus général, rendant compte d'un plus grand nombre de phénomènes.

La notion d'entropie a été généralisée dès la fin du XIXe siècle par la mécanique statistique et notamment par le physicien autrichien Ludwig Boltzmann<sup id="a4">[4](#f4)</sup>. Considérant que chaque état macroscopique d'un gaz peut être réalisé par un certain nombre de « complexions » microscopiques dépendant de la position et de la vélocité des molécules qui le composent, Boltzmann en a déduit que cet état est d'autant plus probable que le nombre des complexions qui peuvent le réaliser est grand. L'entropie est proportionnelle à la probabilité, d'où la formule qu'on a gravée sur son tombeau:
S = *k* log W
où S est l'entropie, W la probabilité de l'état thermodynamique d'un gaz et k une constante dite « constante de Boltzmann » (1,381 x 10-16 C.G.S.).
Nous citons ici la formule de Boltzmann parce que sa structure logarithmique et probabiliste a servi plus tard de modèle aux théoriciens de l'information.

Mouvement irréversible, l'augmentation de l'entropie apparaît donc comme l'évolution d'un ordre différencié vers un désordre indiffé­rencié ou, si l'on préfère, d'une prévisibilité quantifiable vers une imprévi­sibilité aléatoire. Par exemple nous savons que la température d'un gaz est liée à l'énergie cinétique de translation des molécules qui le composent. Si une certaine quantité de ce gaz est répartie entre un espace « chaud » et un espace « froid », il est possible de calculer la vélocité et la position des molécules, c'est-à-dire qu'on peut définir là un système et évaluer son énergie disponible. On peut récupérer celle-ci en faisant passer les molécules de l'espace « chaud » dans l'espace « froid », ce qui produit un travail utilisable moyennant un dispositif mécanique, mais aboutit à une situation d'équilibre où les molécules sont distribuées de manière aléatoire. Bien que leur énergie cinétique totale reste la même, il devient impossible, en vertu du principe de Carnot, de tirer d'elles du travail.

On ne peut le faire qu'à condition de considérer chaque molécule
comme un système organisé à l'intérieur duquel la distribution des atomes n'est pas aléatoire. Pour récupérer l'énergie intra-moléculaire d'un tel système, il faut « faire craquer » les molécules. C'est ce qui se produit dans une réaction chimique, une combustion par exemple - ce qui explique qu'un moteur à combustion interne ait un rendement supérieur à celui d'une machine à vapeur.

On peut aller encore au-delà et « faire craquer » l'organisation interne de l'atome, voire du noyau de l'atome. C'est le principe de l'énergie atomique, puis nucléaire, qui, après récupération du travail (malheureu­sement jusqu'ici par une méthode à faible rendement de réchauffement de fluides), aboutit à un état stable de la matière et, à la limite, à une répartition aléatoire des particules d'énergie.

Dans la mesure où l'univers connu peut être considéré comme une gigantesque machine nucléaire, cette notion sous-tend la plupart des grandes théories cosmogoniques modernes, notamment celles qui supposent une expansion de l'univers à partir d'un noyau initial à l'en­tropie minimale. Certaines de ces théories, pour fuir la fatalité de cette évolution à sens unique, envisagent une alternance cyclique d'expansions et de contractions, auquel cas on pourrait dire que l'univers fonctionne comme une machine de Carnot parfaite à entropie constante à la fin de chacun de ses cycles. D'autres théories, comme celle du physicien anglais Frederick Hoyle, envisagent la création permanente d'atomes dans l'univers et donc une diminution compensative de l'entropie. Les unes et les autres de ces théories font apparaître la notion d'une anti­ entropie à laquelle on a donné le nom de *nég-entropie*.

La biologie a été attirée par la notion d'entropie, puis plus récemment par celle de nég-entropie dans la mesure où elles permettent de rendre compte du processus de l'évolution. En effet une des lois fondamentales de ce processus semble être l'accroissement de la complexité morphologique des espèces, c'est-à-dire l'accroissement de la différenciation, de la spécialisation, de l'organisation, en un mot d'un certain ordre. Le mouvement de la vie serait donc inverse de celui de la matière et correspondrait à une diminution constante de l'entropie. L'évolution darwinienne est en fait nég-entropique<sup id="a5">[5](#f5)</sup>.
Il est tentant d'aller plus loin. Un philosophe français, André
Lalande, l'a fait dès 1899 dans sa thèse de doctorat<sup id="a6">[6](#f6)</sup>, mettant en opposition l'entropie des physiciens et l'évolution des biologistes.

En 1930 il dénonçait les « illusions évolutionnistes» dans les domaines intellectuel, moral et social en proposant la notion d'*involution*<sup id="a7">[7](#f7)</sup>. Il s'agit d'un mouvement entropique caractérisé par un accroissement des similitudes et de l'indifférenciation.

Si elle n'est guère acceptable sous sa forme primitive, l'idée de Lalande est séduisante et présente au moins le mérite de tenter une synthèse du phénomène entropique, Il s'en faut cependant de beaucoup qu'elle puisse être considérée comme une théorie générale. La spécu­lation franchit ici la frontière entre la science et la philosophie.

Sans un outil de quantification adéquat, la transposition de l'entropie et de la nég-entropie dans le domaine des sciences sociales ne peut guère dépasser le stade des métaphores attrayantes. La notion de progrès, par exemple, peut y trouver une nouvelle formulation. Il serait aisé de renouveler brillamment le classique débat scolaire entre Voltaire et Rousseau en disant par exemple que l'un a une vision nég-eritropique et l'autre une vision entropique de l'évolution des institutions humaines. On pourrait aussi montrer le contraire car le raisonnement par métaphore n'a d'autre effet que de masquer les réalités et, en l'occurrence, les réalités idéologiques. Décrire l'histoire, ainsi que l'a fait en d'autres termes Paul Valéry, comme une alternance de périodes entropiques et de périodes nég-entropiques, ne serait qu'une façon de ramener les mouvements révolutionnaires à des épisodes cycliques à l'intérieur d'une machine de Carnot où « plus cela change, plus c'est la même chose ».

C'est cependant peut-être l'idée de *modèle répétitif* qui peut aider
les sciences de l'homme à tirer partie des notions d'entropie et de nég­-entropie. La biologie intracellulaire fait usage de la notion de « programmes » transportés par des « messagers » qui sont les gènes, dans la molécule d'A.D.N. Le programme est l'inscription par le messager, au moyen d'un code chimique, du modèle d'une certaine organisation, d'un certain ordre. Il tend à maintenir au niveau *phylo­génique* (celui de l'espèce) une entropie constante, alors qu'au niveau *ontogénique* la vie de l'individu se caractérise par une période nég­-entropique, suivie d'une période entropique. Mais parfois il y a des incidents de communication qui impliquent des modifications du programme. Ces modifications sont des mutations qui peuvent avoir un caractère nég-entropique comme dans le cas des adaptations au
milieu, ou entropique comme dans le cas du cancer<sup id="a8">[8](#f8)</sup>.

Nous sommes ici ramenés au schéma de la communication au sens où elle est transfert d'information. Il est donc probable que, dès qu'on passe du mécanique au vivant et plus encore du vivant à l'humain, un nouvel outil épistémologique devient nécessaire. Cet outil est sans doute la notion d'information. Nous pouvons hasarder l'hypothèse que cette notion est structurellement liée à celle d'entropie, mais qu'elle en diffère par le fait qu'elle prend en compte deux éléments laissés de côté par la science physique: la vie et surtout la pensée.

### CHAOS ET COSMOS
Toutes les religions cosmogoniques ont décrit la création comme le passage d'un état de désordre indifférencié (le *khaos* des Grecs, le *tohu ve bohu* de la Bible) à un ordre organisé (le *kosmos* des Grecs, le *mondus* des Latins repris par le christianisme). De toute évidence la création est décrite comme un processus nég-entropique.

« Au commencement Eloïm créa la terre et le ciel » est un énoncé sans antériorité et parfaitement imprévisible. Il ne renvoie à rien qu'à une bipartition arbitraire dans un ensemble infiniment homogène et donc infiniment entropique. Il faut insister sur le caractère énonciatif de cette « création ». Ses phases ultérieures sont introduites par « Eloïm *dit* ». C'est en ce sens que les théologiens ont défini l'acte créateur comme Verbe. En fait il y a production par une volonté de deux « signes » le terrestre et le céleste, au sein de ce qui était perçu comme une absence de signification.

Ces signes se définissent par des caractéristiques différentes et exclusives les unes des autres, qui sont des états de la matière. Cela entraîne une diminution brutale de l'entropie au sens probabiliste de Boltzmann. Le nombre des « complexions » qui peuvent réaliser chacun des états désignés par ces signes, n'est plus infini: il est simplement très grand. On entre dans le domaine de l'évaluable, sillon encore du mesurable. Un pas décisif a été franchi, celui de la création : un ordre a été constitué.

Dès lors, les bipartitions s'enchaînent les unes aux autres: la lumière et l'obscurité, les eaux d'en haut et les eaux d'en bas, etc. A chaque étape se construit progressivement une codification binaire de l'univers par création de *sous-ensembles* dont le nombre croît exponentiellement. On observera que, si le nombre des bipartitions effectuées est de *n*, le nombre des sous-ensembles et donc des signes les désignant est de 2^*n*, ce qui oriente naturellement vers une relation logarithmique. Le nombre des « complexions » qui peuvent réaliser l'état correspondant à chaque sous-ensemble est de moins en moins grand, ce qui implique que cet état est de moins en moins probable. On peut imaginer le cas où une infinité de bipartitions aboutit à une probabilité de <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{\infty&space;}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{\infty&space;}" title="\frac{1}{\infty }" /></a> où l'entropie tend vers zéro et où l'univers est entièrement énoncé. C'est peut-être là ce que Teilhard de Chardin appelle le « point oméga ».

On peut aussi, en interprétant strictement la Bible, considérer que le processus nég-entropique ne dure qu'un temps limité (les 7 jours de la création), pour être immédiatement suivi d'un processus entropique de « retour à la poussière », c'est-à-dire à un état indifférencié. Cela s'accorderait assez bien avec les observations de la physique. Cela s'accor­derait aussi avec les spéculations de la biologie si l'on admet que, dans le cas particulier de la matière vivante, le cycle entier de la période nég-entropique suivie d'une période entropique est réalisé au niveau de chaque espèce (phylogenèse) et au niveau de chaque individu (onto­
génèse).
Mais - et c'est là que la théologie fait des suggestions intéressantes à la théorie de l'information - on peut également dire que l'« accident » humain amorce dans une création vouée à l'entropie un contre-processus nég-entropique fondé sur la pensée et donc sur la capacité d'énonciation productive. En mangeant le fruit de l'arbre de la science, Adam et Eve s'emparent du pouvoir divin du verbe qui leur permet de continuer la création à travers la « dé-création ». Ce pouvoir est celui d'annuler l'entropie par la parole, c'est-à-dire par l'émission de signes.

Il est difficile de dire si les théoriciens de l'information ont été inspirés par la Bible, mais il est de fait que *l'annulation de l'entropie par l'émission de signes* est très exactement l'idée qu'ils ont été amenés à se faire de l'information.

Le bon sens a du mal à voir dans l'information, phénomène ressenti comme positif entre tous, comme un facteur négatif, même si ce facteur affecte un processus lui-même à coloration négative, tel que l'entropie. Il est important, pour bien comprendre l'attitude intellectuelle de la théorie de l'information, de ne pas assimiler l'information à des acqui­sitions comme le savoir ou la connaissance, mais de lui maintenir son double signe négatif.

Cette façon de voir les choses fait en somme de l'information une valeur de signe opposé au travail mécanique. De là dérive la tentation de calquer le schéma de sa production sur le schéma de la production de travail mécanique. On peut imaginer une source « froide » chargée d'entropie qui fait passer du « froid » (c'est-à-dire de l'indétermination) sur une source « chaude » et produit au passage de l'information, le processus étant irréversible. La mesure de l'information potentielle contenue dans la source froide est son imprévisibilité statistique.

En 1924, un ingénieur de l'American Telephone and Telegraph Company, Harry Nyquist, publia un article sur la vitesse de trans­mission des messages télégraphiques<sup id="a9">[9](#f9)</sup>. Il montrait que si un système télégraphique dispose de *m* « signes » (c'est-à-dire qu'il peut produire *m* modulations du courant électrique), la vitesse de transmission W est liée à *m* par la relation W = K log *m* où K est une constante dépendant du nombre des modulations que le système peut transmettre par seconde.

On ne peut qu'être frappé par la ressemblance formelle de cette formule avec celle de Boltzmann. En 1929 le physicien L. Szilard utilisa la théorie naissante de l'information pour lever une ambiguïté de la thermodynamique.

Un pas décisif fut franchi en 1928 par un théoricien de la télécommu­nication, R.Y. L. Hartley, qui, pour la première fois, introduisit une *mesure de la quantité d'information*. Si l'on imagine un système disposant de *s* signes équiprobables, un message composé de *n* de ces signes contient
une quantité d'information H telle que :

H = *n* log *s*

Cette formule a été généralisée ultérieurement sous la forme suivante: si un ensemble M contient *m* signes équiprobables, donc de probabilité *p* = <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{\mathit{m}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{\mathit{m}}" title="\frac{1}{\mathit{m}}" /></a> , l'émission de chaque signe comporte une quantité d'information telle que:

<a href="https://www.codecogs.com/eqnedit.php?latex=\textup{H}&space;=&space;\textup{K&space;log&space;}&space;m&space;=&space;-&space;\textup{&space;K&space;log&space;}&space;\frac{1}{m}=-\textup{&space;K&space;log&space;}p" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textup{H}&space;=&space;\textup{K&space;log&space;}&space;m&space;=&space;-&space;\textup{&space;K&space;log&space;}&space;\frac{1}{m}=-\textup{&space;K&space;log&space;}p" title="\textup{H} = \textup{K log } m = - \textup{ K log } \frac{1}{m}=-\textup{ K log }p" /></a>

où K est une constante dépendant des unités et de la base de logarithmes choisies. En prenant pour K la valeur de la constante de Boltzmann et en utilisant des logarithmes népériens, on obtient une valeur formulée en *nats (natural units)* qui permet d'établir une relation mathématique entre la mesure de l'entropie thermodynamique et celle de l'information. Les deux formules sont, au signe près, identiques.

Pour que le parallèle fût complet, il restait à mesurer la quantité d'information potentielle d'une source, c'est-à-dire son entropie. C'est ce qu'a fait Shannon. En supposant une source disposant de *n* signes non nécessairement équiprobables, il a montré que l'entropie informa­tionnelle H de cette source est calculable par la formule :

<a href="https://www.codecogs.com/eqnedit.php?latex=\textup{H}&space;=&space;-&space;\sum_{i=1}^{n}&space;p_i&space;\textup{&space;log&space;}&space;p_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textup{H}&space;=&space;-&space;\sum_{i=1}^{n}&space;p_i&space;\textup{&space;log&space;}&space;p_i" title="\textup{H} = - \sum_{i=1}^{n} p_i \textup{ log } p_i" /></a>

C'est-à-dire que l'entropie d'une source est égale à moins la somme des probabilités de chacun des signes dont dispose la source, multipliées chacune par son propre logarithme.

Le caractère binaire, déjà évoqué ci-dessus, de la constitution de beaucoup de systèmes de signes, notamment lorsqu'on utilise le courant électrique (positif/négatif ou circuit ouvert/circuit fermé), conduit ici à utiliser surtout des *logarithmes de base 2* (2 log n = N) et à exprimer le résultat en *bits* ou *binary units*<sup id="a10">[10](#f10)</sup>. Le *bit* est la quantité d'information contenue par une source disposant de deux signes équiprobables, comme une pièce de monnaie dans le jeu de pile ou face.

On notera que dans le cas d'une source disposant de *n* signes équi­probables, la formule de Shannon peut être simplifiée:

<a href="https://www.codecogs.com/eqnedit.php?latex=\textup{H}&space;=&space;-&space;\sum_{i=1}^{n}&space;p_i&space;\textup{&space;log&space;}&space;p_i=-n(\frac{1}{n}\textup{&space;log&space;}\frac{1}{n})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textup{H}&space;=&space;-&space;\sum_{i=1}^{n}&space;p_i&space;\textup{&space;log&space;}&space;p_i=-n(\frac{1}{n}\textup{&space;log&space;}\frac{1}{n})" title="\textup{H} = - \sum_{i=1}^{n} p_i \textup{ log } p_i=-n(\frac{1}{n}\textup{ log }\frac{1}{n})" /></a>

On vérifie dans le cas du jeu de pile ou face que:

<a href="https://www.codecogs.com/eqnedit.php?latex=\textup{H}&space;=&space;-\textup{&space;log&space;}\frac{1}{2}=-(-1)=1&space;bit" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textup{H}&space;=&space;-\textup{&space;log&space;}\frac{1}{2}=-(-1)=1&space;bit" title="\textup{H} = -\textup{ log }\frac{1}{2}=-(-1)=1 bit" /></a>

Cela signifie que l'entropie de la pièce de monnaie avant d'être lancée est de 1 *bit* et qu'un seul coup de pile ou face suffit à épuiser l'entropie *liée à cette expérience,* puisque, selon la formule de Hartley, il apporte
une quantité d'information de :

<a href="https://www.codecogs.com/eqnedit.php?latex=\textup{H}&space;=&space;n&space;\textup{&space;log&space;}&space;s&space;=&space;1&space;*&space;\textup{log&space;}&space;2&space;=&space;1&space;*&space;1&space;=&space;1&space;bit" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\textup{H}&space;=&space;n&space;\textup{&space;log&space;}&space;s&space;=&space;1&space;*&space;\textup{log&space;}&space;2&space;=&space;1&space;*&space;1&space;=&space;1&space;bit" title="\textup{H} = n \textup{ log } s = 1 * \textup{log } 2 = 1 * 1 = 1 bit" /></a>

Mais si l'on institue une nouvelle expérience, c'est-à-dire si on lance de nouveau la pièce de monnaie, l'entropie de la pièce est de nouveau de 1 *bit*. Le résultat de l'expérience n'affecte pas les probabilités mises en jeu dans l'expérience suivante. Il en serait autrement si le fait d'apparaître dans un coup donné modifiait pour le côté pile ou le côté face sa probabilité d'apparition dans le coup suivant. Il faudrait pour cela que la pièce de monnaie fût dotée d'une *mémoire* lui permettant d'enregistrer les résultats des coups joués et de modifier les probabilités d'apparition des deux signes dont elle dispose en fonction de ces résultats, donc de modifier son entropie.

Mais une pièce de monnaie est une source sans mémoire. On retiendra que *la formule de Shannon s'applique exclusivement à des sources sans mémoire.*

### LE MODÈLE MÉCANISTE

Pour les techniciens de la télécommunication, la formule de Shannon est un aboutissement et un point de départ. Le problème de la quanti­fication de l'information est, sinon entièrement résolu, du moins ramené à un modèle qui, moyennant l'inversion d'un signe, est analogue à celui de la quantification de l'énergie. Cela permet d'inclure dans un même schéma la théorie du vecteur (qu'on appellera théorie de la communi­cation) et la théorie de l'information proprement dite. Ce schéma classique est celui auquel se réfèrent pratiquement tous les théoriciens de l'infor­mation. Sous sa forme la plus développée il se présente ainsi:

![alt text](https://github.com/belzepaf/Theorie_Generale/blob/master/TGSIC/figures/figures/schema1.JPG "Fig.1 Schéma linéaire de la communication")
*Figure* 1. __Le schéma linéaire de la communication__

Le transfert d'information s'effectue de la *source* au *destinataire* entre lesquels il y a une diminution de l'entropie informationnelle. Le transfert d'énergie vectrice s'effectue de l'*émetteur* au *récepteur* entre lesquels il y a une augmentation de l'entropie énergétique. Le *codeur* inscrit dans le vecteur énergétique les modulations issues de la source, le *décodeur* les identifie et les transmet au destinataire. Entre l'émetteur et le récepteur la *voie*, qui est tout ou partie du conducteur, transporte l'énergie modulée. Le *canal* est l'ensemble du dispositif situé entre la sortie de la source et l'entrée du destinataire.

Ce schéma ne s'applique pas seulement à la télécommunication électrique, Quand nous parlons à quelqu'un par exemple, une partie du cerveau, située dans le cortex, sert de source, une autre partie, située dans la zone temporale de l'hémisphère gauche (chez les droitiers), sert de codeur, Les impulsions venues du centre de codage viennent moduler l'énergie acoustique produite par un appareil qui implique le système musculaire, l'appareil respiratoire et l'appareil de phonation. L'énergie modulée est transportée sur une voie qui est l'air ambiant, elle est captée par un récepteur constitué par l'oreille externe, le tympan (qui est un transformateur d'énergie), le conducteur mécanique des osselets et l'oreille interne, qui achemine les modulations vers le centre de décodage par le nerf auditif. Les modulations décodées sont alors reçues par le destinataire qui est situé dans le cortex de l'auditeur.

Cet exemple permet de mettre en lumière les facteurs qui peuvent affecter la transmission de l'information. Les uns sont de nature éner­gétique, les autres de nature informationnelle. Leur caractéristique commune est de se produire dans le canal, mais à des niveaux différents.

L'incident énergétique le plus banal est l'affaiblissement de l'énergie vectrice, entraînant une « perte de signal » quand par exemple on est hors de portée de voix. L'aphonie ou la surdité qui sont des « pannes » d'émetteur ou de récepteur sont des incidents du même ordre.

Il peut arriver aussi qu'un train d'énergie extérieure emprunte accidentellement la même voie que l'énergie vectrice et perturbe ou efface ses modulations. C'est ce qui se produit par exemple quand le passage en rase-mottes d'un avion à réaction interrompt ou gêne une conversation. Dans certains cas, même, ce train d'énergie parasite est lui-même vecteur de modulations informationnelles qui viennent déformer l'information originelle ou même se substituer à elle, comme par exemple quand plusieurs conversations téléphoniques se trouvent branchées sur une même ligne. En effet ce type de perturbations n'est pas seulement le fait de la communication orale directe. On en trouve l'équivalent dans tous les systèmes de communication de tous les types : les vieux amateurs de T.S.F. connaissaient bien le problème des parasites.

Dans tous les cas et quel que soit le mode d'énergie vectrice employée, on donne à ces phénomènes perturbatoires le nom générique de bruits. Beaucoup de bruits sont accidentels et dus au fonctionnement du canal, mais d'autres sont intentionnels (les brouillages) et certains peuvent même délibérément véhiculer une information parasite comme les messages subliminaux de la publicité ou de la propagande.

L'étude du *canal bruyant* est donc un des chapitres majeurs de la théorie de la communication. Ajoutons qu'il comporte un problème technique particulier qui est celui de la *bande passante*. En effet l'énergie vectrice étant transmise sous la forme d'une modulation périodique, elle n'est pas toujours capable d'inscrire toutes les modulations informa­tionnelles. Par exemple quand les modulations de la voix humaine sont inscrites dans le courant électrique passant dans un fil téléphonique, seules sont réellement inscrites celles qui ne dépassent pas les limites supérieures et inférieures définissant la *bande passante* du courant vecteur. Au décodage on ne retrouve donc pas toute l'information, et la voix est perçue comme déformée. Dans un poste de radio, on peut faire varier le niveau de la bande passante en appuyant sur le bouton des graves ou sur celui des aigus. Pour la réception des messages musicaux où la perception quasi totale des modulations est indispensable, on cherche à utiliser des canaux sans bruit comme ceux des chaînes de haute­ fidélité ou de la modulation de fréquence.

Mais les dispositifs permettant d'obtenir des canaux sans bruit sont coûteux et, rappelons-le, les théoriciens de l'information ont eu sans cesse en tête des problèmes de rentabilité. La théorie de l'information permettant une mesure quantitative .du bruit, ils ont cherché d'autres méthodes pour éliminer les erreurs (c'est-à-dire les perturbations informa­tionnelles) que peut causer le bruit. Un des mérites de Shannon, plus grand encore que celui de sa mesure de l'entropie, est d'avoir formulé ce qui s'appelle le « théorème de canal bruyant ». Ce théorème, dont il sera question plus loin, est fondé sur une utilisation du codage : les défauts de la chaîne énergétique sont corrigés par une amélioration. du rendement de la chaîne informationnelle.

Le codage a des incidents comme la transmission énergétique. Le codeur et le décodeur peuvent tomber « en panne ». Dans le cas de la communication orale directe, c'est ce qui arrive quand un traumatisme, par exemple, affecte les centres cérébraux de l'audition ou de la phonation. On a alors une « surdité verbale » ou une aphasie qui sont des incapacités à formuler ou à identifier les modulations, alors que la surdité tout court ou l'aphonie sont des incapacités mécaniques à émettre l'énergie modulée.

Mais - et c'est là qu'on arrive à une des limites du schéma mécaniste de la communication - les problèmes du codage et du décodage sont très différents de ceux de la transmission ou de la transformation de l'énergie et en fait d'une tout autre nature.

Il nous faut, pour comprendre cela, revenir au principe même de l'analogie entre le processus énergétique et le processus informationnel.

Une source énergétique « chaude » à entropie faible possède une organisation *réelle* décelable à divers niveaux (moléculaire, atomique) et par divers effets (mouvements, température, ondes du spectre perceptible pour l'organisme humain). Entre ces effets il existe des équivalents immuables ou du moins suffisamment stables pour fonder une prévisibilité statistique. Les transformations s'opèrent selon des lois rigoureuses, dites « naturelles », que l'esprit humain formule progres­sivement (en général par approximation et au besoin grâce à des hypothèses qui devancent l'expérience), mais sur lesquelles il n'a pas pouvoir de décision.

Au contraire une source informationnelle qualifiée par métaphore de « froide » a par définition un haut degré d'entropie : elle est indéterminée, indifférenciée. Elle ne peut confier au canal des séquences de messages que dans la mesure où on lui a *supposé* des éléments *fictifs*, ce qui implique qu'elle est *observée* par un esprit possédant le pouvoir d'énonciation signalé plus haut comme le pouvoir « créateur » privilège de la divinité, et par voie de conséquence, de la pensée humaine.

Quand la source est une source énergétique physique, on retrouve le cas de l'hypothèse de type scientifique, permettant de formuler des lois de plus en plus approchées de l'expérience au moyen d'éléments de plus en plus « fins ». Mais quand la source est de nature supposée divine (ce qui est le cas de la pensée pré-scientifique ou magique)" ou plus généralement humaine (voire, dans certaines religions, anthropo­morphique), il est bien évident que les éléments fictifs doivent faire l'objet d'une *convention préalable* entre le codeur et le décodeur, sans quoi les séquences de modulations qui les inscriront dans le vecteur énergétique ne seront pas reconnues par le décodeur comme des messages.

Cette convention s'appelle un *code*. Un code est une liste de *signes*. Nous appellerons signes les éléments énoncés par convention comme existant dans une source informationnelle. Tout signe est un *biface*, une de ces faces étant le *signifiant*, c'est-à-dire la modulation qui l'inscrit dans le vecteur, l'autre étant le *signifié*, c'est-à-dire la définition conven­tionnelle de l'incertitude qui est annulée par l'apparition du signe.

La *théorie du signe* fait l'objet de plusieurs disciplines, notamment la sémantique et la sémiologie. Il en sera question plus loin.

Les théoriciens de la télécommunication s'intéressent avant tout au signifiant qui doit avoir un certain nombre de qualités: résistance au bruit, facilité de codage et de décodagevvitesse de transmission. Ils ne s'intéressent au signifié que dans la mesure où ses caractéristiques ont une incidence sur celles du signifiant.

L'un de ces théoriciens, le physicien français Élie Roubine, comparant d'une part l'attitude de l'expéditeur et du destinataire d'un télégramme, d'autre part celle d'un employé des Postes, souligne que les premiers sont avant tout concernés par la signification. Au contraire « le point de vue de l'employé des Postes est quantitatif. *La signification des messages transmis lui est indifférente*, (...) Son rôle est de faire payer un service : la transmission d'une « quantité d'information » (...) propor­tionnelle à la longueur du texte »<sup id="a11">[11](#f11)</sup>.

Nous aurons à revenir sur le rôle de l'employé des Postes comme représentant de ce que nous appellerons le « producteur », c'est-à-dire le contrôleur du canal (en l'occurrence l'administrateur des P. et T.). Nous aurons alors à nous demander si son souci financier n'introduit pas un « bruit » informationnel dans la communication.

Élie Roubine ajoute: « Cette optique est précisément celle de la théorie de l'information. La signification des messages n'est pas prise en considération. Un texte incohérent, c'est-à-dire toute suite de caractères, apparemment arbitraire, a valeur de message. Il n'y a aucun inconvénient à identifier texte et message »<sup id="a12">[12](#f12)</sup>.

On ne saurait mieux définir les limites du modèle mécaniste de la théorie de l'information qui, en ce cas, n'est en fait qu'une théorie de la communication, comme d'ailleurs l'avait nommée Shannon à ses origines : *theory of communication*. Ce qui est dit du texte écrit s'applique aussi bien au discours oral, c'est-à-dire à tout produit du langage. Or le langage est à l'origine de tout codage et à l'aboutissement de tout décodage dans la mesure où tout fait de communication suppose en un point quelconque du système l'intervention différée ou non d'individus pensants et parlants, donc doués du pouvoir d'énonciation signalé comme le propre de la divinité, mais surtout de l'homme<sup id="a13">[13](#f13)</sup>.

***
<b id="f1">1</b> : Sadi Carnot (1796-1832), fils du « Grand Carnot» et oncle du futur Président de la République, ancien polytechnicien, a participé comme officier du génie aux derniers combats de l'Empire. Sa brochure, tirée à 200 exemplaires, a longtemps été ignorée et sa portée n'en fut comprise que bien des années plus tard par les physiciens Kelvin et Clapeyron.[↩](#a1)

<b id="f2">2</b> : C'est dans son ouvrage *Uber die bewegende Kraft der Wiirme* (1850) que Clausius, alors professeur de physique à l'école d'artillerie de Berlin, a reformulé le principe de Carnot.[↩](#a2)

<b id="f3">3</b> : L'Ecossais James Watt (1736-1819) était avant tout un ingénieur praticien. C'est en 1767 qu'il réalisa la première machine à vapeur opérationnelle et en 1776 la machine à double effet grâce, notamment, au dispositif du tiroir.[↩](#a3)

<b id="f4">4</b> : Ludwig Boltzmann (1844-1906) est à l'origine de nombreuses théories mathéma­tiques qui ont joué un rôle décisif dans l'évolution de la physique nucléaire et atomique, notamment la *théorie des quanta*.[↩](#a4)

<b id="f5">5</b> : On trouvera une excellente discussion de ce problème dans l'article « The Concept of Evolution » du chapitre « Evolution » (par R.C. Lewontin) de l'*International Encyclopedia of the Social Sciences*, vol. 5, pp. 202-210.[↩](#a5)

<b id="f6">6</b> : Le titre en est *L'idée directrice de dissolution opposée à celle d'évolution dans la méthode des sciences physiques et morales*. Lalande est célèbre pour son *Vocabulaire philo­sophique* (1902-1923).[↩](#a6)

<b id="f7">7</b> : Dans *Les illusions évolutionnistes* (1930).[↩](#a7)

<b id="f8">8</b> : L'A.D.N. est l'*acide désoxyribonucléique* qui est le constituant de base du noyau cellulaire et des chromosomes. Le *gène*, élément chimique autocatalytique (c'est-à-dire susceptible de se reproduire), est porteur des éléments du patrimoine héréditaire.[↩](#a8)

<b id="f9">9</b> : « Certain Factors Affecting Telegraph Speed », cité par J.R. PIERCE, *Symbols, Signs and Noise*, New York, Harper, 1961, p. 35.[↩](#a9)

<b id="f10">10</b> : Le terme de bit a été introduit par Shannon. II présente l'inconvénient de désigner à la fois une unité d'information et un symbole numérique (1 et 0) dans la notation binaire. On a proposé pour lui au sens d'unité d'information les noms de shannon ou de logon, mais l'usage semble imposer *bit*. Le *bit* correspond à l'emploi de logarithmes de base 2, le *nat* à celui de logarithmes naturels ou népériens (base *e*) et le *decit* ou *hartley* à celui de logarithmes décimaux.[↩](#a10)

<b id="f11">11</b> : Elie ROUBINE, *Introduction à la théorie de la communication*, Paris, Masson 1970, tome III, *Théorie de l'information*, p. 2.[↩](#a11)

<b id="f12">12</b> : *Ibidem*.[↩](#a12)

<b id="f13">13</b> : Les problèmes généraux soulevés dans ce chapitre au sujet de la liaison entre la thermodynamique et l'information ont été traités par le physicien français Léon Brillouin dans *La Science et la théorie de l'information*, Paris, Masson, 1959. Pour une critique des idées de Brillouin, voir l'article du physicien A. Marchand (Université de Bordeaux I) « Sur le coût minimum de l'information. Comparaison du Principe de Carnot et du Principe de Carnot généralisé de Brillouin» dans *Le Journal de Physique*, 1976, 37, pp. 297-301.[↩](#a13)
